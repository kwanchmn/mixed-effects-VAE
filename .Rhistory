test_clusters = setdiff(unique_clusters, train_clusters)
return(list(train = train_clusters, test = test_clusters))
}
# Sampling clusters in training data
train_subject_n = 30
cluster_split = train_test_clusters(mrl_train_df[["subject"]], train_subject_n)
# Subsetting training data with above cluster list
mrl_train_df_subset = mrl_train_df %>%
filter(subject %in% cluster_split$train)
# ===============================
# Down-sampling original mrl data
# ===============================
data_downsampler = function(data, cluster_colname, cluster_size) {
# Downsizes the original data
# Parameters
# data: the dataframe to be downsized
# cluster_size: the number of observations to be included in each cluster
data_subset = data %>%
group_by(across(cluster_colname)) %>%
slice_sample(n = cluster_size)
return(data_subset)
}
# Downsizing subset mrl train data
subject_obs_n = 100
mrl_train_downsized = data_downsampler(mrl_train_df_subset, "subject", subject_obs_n)
head(mrl_train_downsized)
# ===============================
# Create latent variables sampler
# ===============================
latent_dim = 2
latent_samp_R = 100
# Within-cluster correlation
rho_h = 0.5  # Hyper-parameter
within_cluster_cov = diag(rho_h, latent_dim)
# mu and Sigma of cluster i
cov_mat_h_i = matrix(rep(1, subject_obs_n**2), nrow = subject_obs_n, byrow = TRUE) %x% within_cluster_cov
diag(cov_mat_h_i) = 1
mean_h_i = rep(0, times = latent_dim * subject_obs_n)
# Drawing samples of h_i
samples_list <- replicate(latent_samp_R, {
h_i_samp = mvrnorm(mu = mean_h_i, Sigma = cov_mat_h_i)
h_i_samp = matrix(h_i_samp, nrow = subject_obs_n, ncol = latent_dim, byrow = TRUE)
}, simplify = FALSE)
# Checking if the sample mean and covariance matrix are similar to population one
col_mean = Reduce("+", samples_list) / latent_samp_R
max(abs(col_mean - mean_h_i))
# And the covariance matrix
# Flatten each 100x2 sample back into a 200-length vector using byrow = TRUE
samples_matrix <- t(sapply(samples_list, function(mat) as.vector(t(mat))))
# Compute the 200x200 sample covariance matrix
sample_cov <- cov(samples_matrix)
max(abs(sample_cov - cov_mat_h_i))
# ===============================
# Glue simulated latent variables as response
# ===============================
mrl_train_downsized = subset(mrl_train_downsized, select = c("filename", "state", "subject"))
for (r in seq(1, latent_samp_R)) {
latent_z_samp = replicate(train_subject_n, {
h_i_samp = mvrnorm(mu = mean_h_i, Sigma = cov_mat_h_i)
h_i_samp = matrix(h_i_samp, nrow = subject_obs_n, ncol = latent_dim, byrow = TRUE)
}, simplify = FALSE)
latent_z_samp = do.call(rbind, latent_z_samp)
latent_z_samp = lapply(split(latent_z_samp, seq(nrow(latent_z_samp))), as.numeric)
mrl_train_downsized[[paste0("z_", r)]] = latent_z_samp
}
head(mrl_train_downsized)
# ===============================
# Building the neural network
# ===============================
resnet = application_resnet50_v2(include_top = FALSE,
weights = 'imagenet',
input_shape = c(100, 100, 3))
resnet$trainable = FALSE
# Setting up layers up to the latent variables as output
flatten_layer = layer_flatten()
flatten_layer$build(shape(NULL, 4, 4, 2048))
## Add a drop-out layer, and l1 penalty
dropout_rate = 0.99
dropout_layer = layer_dropout(rate = dropout_rate)
output_layer = layer_dense(units = 2)
output_layer$build(shape(NULL, 4 * 4 * 2048))
# Early stopping
callbacks = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
# Building the sequential model
model = keras_model_sequential(c(100, 100, 3))
model %>%
resnet %>%
flatten_layer %>%
dropout_layer %>%
output_layer
model %>% compile(optimizer = "adam",
loss = 'mean_squared_error',
metrics = list('mse'),
callbacks = callbacks)
# Reading image
img_path <- filepath_list[1]
img <- load.image(img_path)
img_resized <- imresize(img)
plot(img_resized)
# ===============================
# Load Libraries
# ===============================
library(keras3)
library(lmerTest)
library(nlme)
library(tidyverse)
library(MVN)
library(MASS)
library(docstring)
library(imager)
# Reproducibility
seed = 114514
set.seed(seed)
# ===============================
# Load data
# ===============================
mrl_train_df = read_csv("metadata/train_img_metadata.csv", col_select = -1)
mrl_val_df = read_csv("metadata/val_img_metadata.csv", col_select = -1)
mrl_test_df = read_csv("metadata/test_img_metadata.csv", col_select = -1)
# ===============================
# Splitting train and test clusters
# ===============================
train_test_clusters = function(cluster_list, train_subject_obs_n) {
# Randomly selects which clusters to be included in the training data
# Parameters
# cluster_list: a vector containing the grouping factor in the data
# train_subject_obs_n: number of clusters to be included in the training data, must be equal to or less than the total number of clusters
unique_clusters = unique(cluster_list)
train_clusters = sample(unique_clusters, train_subject_obs_n)
test_clusters = setdiff(unique_clusters, train_clusters)
return(list(train = train_clusters, test = test_clusters))
}
# Sampling clusters in training data
train_subject_n = 30
cluster_split = train_test_clusters(mrl_train_df[["subject"]], train_subject_n)
# Subsetting training data with above cluster list
mrl_train_df_subset = mrl_train_df %>%
filter(subject %in% cluster_split$train)
# ===============================
# Down-sampling original mrl data
# ===============================
data_downsampler = function(data, cluster_colname, cluster_size) {
# Downsizes the original data
# Parameters
# data: the dataframe to be downsized
# cluster_size: the number of observations to be included in each cluster
data_subset = data %>%
group_by(across(cluster_colname)) %>%
slice_sample(n = cluster_size)
return(data_subset)
}
# Downsizing subset mrl train data
subject_obs_n = 100
mrl_train_downsized = data_downsampler(mrl_train_df_subset, "subject", subject_obs_n)
head(mrl_train_downsized)
# ===============================
# Create latent variables sampler
# ===============================
latent_dim = 2
latent_samp_R = 100
# Within-cluster correlation
rho_h = 0.5  # Hyper-parameter
within_cluster_cov = diag(rho_h, latent_dim)
# mu and Sigma of cluster i
cov_mat_h_i = matrix(rep(1, subject_obs_n**2), nrow = subject_obs_n, byrow = TRUE) %x% within_cluster_cov
diag(cov_mat_h_i) = 1
mean_h_i = rep(0, times = latent_dim * subject_obs_n)
# Drawing samples of h_i
samples_list <- replicate(latent_samp_R, {
h_i_samp = mvrnorm(mu = mean_h_i, Sigma = cov_mat_h_i)
h_i_samp = matrix(h_i_samp, nrow = subject_obs_n, ncol = latent_dim, byrow = TRUE)
}, simplify = FALSE)
# Checking if the sample mean and covariance matrix are similar to population one
col_mean = Reduce("+", samples_list) / latent_samp_R
max(abs(col_mean - mean_h_i))
# And the covariance matrix
# Flatten each 100x2 sample back into a 200-length vector using byrow = TRUE
samples_matrix <- t(sapply(samples_list, function(mat) as.vector(t(mat))))
# Compute the 200x200 sample covariance matrix
sample_cov <- cov(samples_matrix)
max(abs(sample_cov - cov_mat_h_i))
# ===============================
# Glue simulated latent variables as response
# ===============================
mrl_train_downsized = subset(mrl_train_downsized, select = c("filename", "state", "subject"))
for (r in seq(1, latent_samp_R)) {
latent_z_samp = replicate(train_subject_n, {
h_i_samp = mvrnorm(mu = mean_h_i, Sigma = cov_mat_h_i)
h_i_samp = matrix(h_i_samp, nrow = subject_obs_n, ncol = latent_dim, byrow = TRUE)
}, simplify = FALSE)
latent_z_samp = do.call(rbind, latent_z_samp)
latent_z_samp = lapply(split(latent_z_samp, seq(nrow(latent_z_samp))), as.numeric)
mrl_train_downsized[[paste0("z_", r)]] = latent_z_samp
}
head(mrl_train_downsized)
# ===============================
# Building the neural network
# ===============================
resnet = application_resnet50_v2(include_top = FALSE,
weights = 'imagenet',
input_shape = c(100, 100, 3))
resnet$trainable = FALSE
# Setting up layers up to the latent variables as output
flatten_layer = layer_flatten()
flatten_layer$build(shape(NULL, 4, 4, 2048))
## Add a drop-out layer, and l1 penalty
dropout_rate = 0.99
dropout_layer = layer_dropout(rate = dropout_rate)
output_layer = layer_dense(units = 2)
output_layer$build(shape(NULL, 4 * 4 * 2048))
# Early stopping
callbacks = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
# Building the sequential model
model = keras_model_sequential(c(100, 100, 3))
model %>%
resnet %>%
flatten_layer %>%
dropout_layer %>%
output_layer
model %>% compile(optimizer = "adam",
loss = 'mean_squared_error',
metrics = list('mse'),
callbacks = callbacks)
# # Reading image
img_path <- r"{mrl-eye-dataset\versions\4\data\train\awake\s0001_01842_0_0_1_0_0_01.png}"
# ===============================
# Loading images as input
# ===============================
filepath_list = paste0(r"{mrl-eye-dataset\versions\4\data\train\}",
mrl_train_downsized$state,
r"{\}",
mrl_train_downsized$filename)
mrl_train_downsized$filename = filepath_list
head(mrl_train_downsized)
head(mrl_train_downsized)
View(mrl_train_downsized)
# # Reading image
img_path <- r"{mrl-eye-dataset\versions\4\data\train\awake\s0001_01842_0_0_1_0_0_01.png}"
img <- image_load(img_path, target_size = c(100, 100, 3))
x <- image_to_array(img)
dim(mrl_test_df)[1]
batch_generator <- function(df, response, target_size = c(100, 100)) {
function() {
# # sample a batch
# batch_indices <- sample(nrow(df), batch_size, replace = TRUE)
# batch_df <- df[batch_indices, ]
# Load images on the fly
n_obs = dim(df)[1]
x_batch <- array(0, dim = c(n_obs, target_size[1], target_size[2], 3))
y_batch <- response
for (i in seq_len(n_obs)) {
img <- image_load(df$filepath[i], target_size = target_size)
img_array <- image_to_array(img) / 255
x_batch[i,,,] <- img_array
}
list(x_batch, y_batch)
}
}
# Early stopping
callbacks_list = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
model %>% compile(optimizer = "adam",
loss = 'mean_squared_error',
metrics = list('mse'),
callbacks = callbacks_list)
model %>% compile(optimizer = "adam",
loss = 'mean_squared_error',
metrics = list('mse'),
callbacks = callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
?compile
# Early stopping
callbacks_list = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
# ===============================
# Load Libraries
# ===============================
library(keras3)
library(lmerTest)
library(nlme)
library(tidyverse)
library(MVN)
library(MASS)
library(docstring)
library(imager)
# Reproducibility
seed = 114514
set.seed(seed)
# ===============================
# Load data
# ===============================
mrl_train_df = read_csv("metadata/train_img_metadata.csv", col_select = -1)
mrl_val_df = read_csv("metadata/val_img_metadata.csv", col_select = -1)
mrl_test_df = read_csv("metadata/test_img_metadata.csv", col_select = -1)
# ===============================
# Splitting train and test clusters
# ===============================
train_test_clusters = function(cluster_list, train_subject_obs_n) {
# Randomly selects which clusters to be included in the training data
# Parameters
# cluster_list: a vector containing the grouping factor in the data
# train_subject_obs_n: number of clusters to be included in the training data, must be equal to or less than the total number of clusters
unique_clusters = unique(cluster_list)
train_clusters = sample(unique_clusters, train_subject_obs_n)
test_clusters = setdiff(unique_clusters, train_clusters)
return(list(train = train_clusters, test = test_clusters))
}
# Sampling clusters in training data
train_subject_n = 30
cluster_split = train_test_clusters(mrl_train_df[["subject"]], train_subject_n)
# Subsetting training data with above cluster list
mrl_train_df_subset = mrl_train_df %>%
filter(subject %in% cluster_split$train)
# ===============================
# Down-sampling original mrl data
# ===============================
data_downsampler = function(data, cluster_colname, cluster_size) {
# Downsizes the original data
# Parameters
# data: the dataframe to be downsized
# cluster_size: the number of observations to be included in each cluster
data_subset = data %>%
group_by(across(cluster_colname)) %>%
slice_sample(n = cluster_size)
return(data_subset)
}
# Downsizing subset mrl train data
subject_obs_n = 100
mrl_train_downsized = data_downsampler(mrl_train_df_subset, "subject", subject_obs_n)
head(mrl_train_downsized)
# ===============================
# Create latent variables sampler
# ===============================
latent_dim = 2
latent_samp_R = 100
# Within-cluster correlation
rho_h = 0.5  # Hyper-parameter
within_cluster_cov = diag(rho_h, latent_dim)
# mu and Sigma of cluster i
cov_mat_h_i = matrix(rep(1, subject_obs_n**2), nrow = subject_obs_n, byrow = TRUE) %x% within_cluster_cov
diag(cov_mat_h_i) = 1
mean_h_i = rep(0, times = latent_dim * subject_obs_n)
# Drawing samples of h_i
samples_list <- replicate(latent_samp_R, {
h_i_samp = mvrnorm(mu = mean_h_i, Sigma = cov_mat_h_i)
h_i_samp = matrix(h_i_samp, nrow = subject_obs_n, ncol = latent_dim, byrow = TRUE)
}, simplify = FALSE)
# Checking if the sample mean and covariance matrix are similar to population one
col_mean = Reduce("+", samples_list) / latent_samp_R
max(abs(col_mean - mean_h_i))
# And the covariance matrix
# Flatten each 100x2 sample back into a 200-length vector using byrow = TRUE
samples_matrix <- t(sapply(samples_list, function(mat) as.vector(t(mat))))
# Compute the 200x200 sample covariance matrix
sample_cov <- cov(samples_matrix)
max(abs(sample_cov - cov_mat_h_i))
# ===============================
# Glue simulated latent variables as response
# ===============================
mrl_train_downsized = subset(mrl_train_downsized, select = c("filename", "state", "subject"))
for (r in seq(1, latent_samp_R)) {
latent_z_samp = replicate(train_subject_n, {
h_i_samp = mvrnorm(mu = mean_h_i, Sigma = cov_mat_h_i)
h_i_samp = matrix(h_i_samp, nrow = subject_obs_n, ncol = latent_dim, byrow = TRUE)
}, simplify = FALSE)
latent_z_samp = do.call(rbind, latent_z_samp)
latent_z_samp = lapply(split(latent_z_samp, seq(nrow(latent_z_samp))), as.numeric)
mrl_train_downsized[[paste0("z_", r)]] = latent_z_samp
}
head(mrl_train_downsized)
# ===============================
# Building the neural network
# ===============================
resnet = application_resnet50_v2(include_top = FALSE,
weights = 'imagenet',
input_shape = c(100, 100, 3))
resnet$trainable = FALSE
# Setting up layers up to the latent variables as output
flatten_layer = layer_flatten()
flatten_layer$build(shape(NULL, 4, 4, 2048))
## Add a drop-out layer, and l1 penalty
dropout_rate = 0.99
dropout_layer = layer_dropout(rate = dropout_rate)
output_layer = layer_dense(units = 2)
output_layer$build(shape(NULL, 4 * 4 * 2048))
# Building the sequential model
model = keras_model_sequential(c(100, 100, 3))
model %>%
resnet %>%
flatten_layer %>%
dropout_layer %>%
output_layer
model %>% compile(optimizer = "adam",
loss = 'mean_squared_error',
metrics = list('mse'))
# ===============================
# Loading images as input
# ===============================
filepath_list = paste0(r"{mrl-eye-dataset\versions\4\data\train\}",
mrl_train_downsized$state,
r"{\}",
mrl_train_downsized$filename)
mrl_train_downsized$filename = filepath_list
head(mrl_train_downsized)
train_images_demo = flow_images_from_directory(
directory = ""
)
# Training
model %>% fit_generator(
generator = batch_generator(mrl_train_downsized, mrl_train_downsized$z_1),
epochs = 30,
callbacks = callbacks_list
)
head(mrl_train_downsized)
batch_generator <- function(df, response, target_size = c(100, 100)) {
function() {
# # sample a batch
# batch_indices <- sample(nrow(df), batch_size, replace = TRUE)
# batch_df <- df[batch_indices, ]
# Load images on the fly
n_obs = dim(df)[1]
x_batch <- array(0, dim = c(n_obs, target_size[1], target_size[2], 3))
y_batch <- response
for (i in seq_len(n_obs)) {
img <- image_load(df$filepath[i], target_size = target_size)
img_array <- image_to_array(img) / 255
x_batch[i,,,] <- img_array
}
list(x_batch, y_batch)
}
}
# Early stopping
callbacks_list = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
# Training
model %>% fit_generator(
generator = batch_generator(mrl_train_downsized, mrl_train_downsized$z_1),
epochs = 30,
callbacks = callbacks_list
)
batch_generator <- function(df, response, target_size = c(100, 100)) {
function() {
# # sample a batch
# batch_indices <- sample(nrow(df), batch_size, replace = TRUE)
# batch_df <- df[batch_indices, ]
# Load images on the fly
n_obs = dim(df)[1]
x_batch <- array(0, dim = c(n_obs, target_size[1], target_size[2], 3))
y_batch <- response
for (i in seq_len(n_obs)) {
img <- image_load(df$filepath[i], target_size = target_size)
img_array <- image_to_array(img) / 255
x_batch[i,,,] <- img_array
}
list(x_batch, y_batch)
}
}
# Early stopping
callbacks_list = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
batch_generator(mrl_train_downsized, mrl_train_downsized$z_1)$x_batch
batch_generator(mrl_train_downsized, mrl_train_downsized$z_1)
batch_generator <- function(df, response, target_size = c(100, 100)) {
# # sample a batch
# batch_indices <- sample(nrow(df), batch_size, replace = TRUE)
# batch_df <- df[batch_indices, ]
# Load images on the fly
n_obs = dim(df)[1]
x_batch <- array(0, dim = c(n_obs, target_size[1], target_size[2], 3))
y_batch <- response
for (i in seq_len(n_obs)) {
img <- image_load(df$filepath[i], target_size = target_size)
img_array <- image_to_array(img) / 255
x_batch[i,,,] <- img_array
}
list(x_batch, y_batch)
}
batch_generator(mrl_train_downsized, mrl_train_downsized$z_1)
batch_generator <- function(df, response, target_size = c(100, 100)) {
# # sample a batch
# batch_indices <- sample(nrow(df), batch_size, replace = TRUE)
# batch_df <- df[batch_indices, ]
# Load images on the fly
n_obs = dim(df)[1]
x_batch <- array(0, dim = c(n_obs, target_size[1], target_size[2], 3))
y_batch <- response
for (i in seq_len(n_obs)) {
img <- image_load(df$filename[i], target_size = target_size)
img_array <- image_to_array(img) / 255
x_batch[i,,,] <- img_array
}
list(x_batch, y_batch)
}
# Early stopping
callbacks_list = list(callback_early_stopping(patience = 5,
restore_best_weights = TRUE))
# Training
model %>% fit(
x = batch_generator(mrl_train_downsized, mrl_train_downsized$z_1)$x_batch,
y = mrl_train_downsized$z_1
epochs = 30,
# Training
model %>% fit(
x = batch_generator(mrl_train_downsized, mrl_train_downsized$z_1)$x_batch,
y = mrl_train_downsized$z_1,
epochs = 30,
callbacks = callbacks_list
)
reticulate::repl_python()
reticulate::repl_python()
